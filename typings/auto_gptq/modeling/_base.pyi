from typing import Any
from typing import Dict
from typing import List
from typing import Optional
from typing import Union

import torch
from _typeshed import Incomplete
from torch import nn
from transformers import PreTrainedModel
from transformers.utils.hub import PushToHubMixin

from ._const import *
from ._utils import *
from ..nn_modules._fused_base import FusedBaseAttentionModule
from ..nn_modules._fused_base import FusedBaseMLPModule

class BaseQuantizeConfig(PushToHubMixin):
    bits: int
    group_size: int
    damp_percent: float
    desc_act: bool
    sym: bool
    true_sequential: bool
    model_name_or_path: Optional[str]
    model_file_base_name: Optional[str]
    def __post_init__(self) -> None: ...
    def save_pretrained(self, save_dir: str, **kwargs: Any) -> None: ...
    @classmethod
    def from_pretrained(cls, save_dir: str, **kwargs: Any) -> BaseQuantizeConfig: ...
    def to_dict(self) -> Dict[str, Any]: ...
    def __init__(self, *, bits: int, group_size: int, damp_percent: float, desc_act: bool, sym: bool, true_sequential: bool, model_name_or_path: Optional[str], model_file_base_name: Optional[str]) -> None: ...

class BaseGPTQForCausalLM(nn.Module, PushToHubMixin):
    layer_type: str
    layers_block_name: str
    outside_layer_modules: List[str]
    inside_layer_modules: List[List[str]]
    lm_head_name: str
    fused_attn_module_type: Optional[FusedBaseAttentionModule]
    fused_mlp_module_type: Optional[FusedBaseMLPModule]
    model: Incomplete
    model_type: Incomplete
    quantize_config: Incomplete
    config: Incomplete
    is_triton_backend: Incomplete
    injected_fused_attention: Incomplete
    injected_fused_mlp: Incomplete
    trainable: Incomplete
    def __init__(self, model: PreTrainedModel, quantized: bool, quantize_config: BaseQuantizeConfig, is_triton_backend: bool = ..., injected_fused_attention: bool = ..., injected_fused_mlp: bool = ..., trainable: bool = ...) -> None: ...
    @property
    def quantized(self) -> bool: ...
    @property
    def hf_device_map(self) -> Any: ...
    module: Incomplete
    data_device: Incomplete
    def quantize(self, examples: List[Dict[str, Union[List[int], torch.LongTensor]]], batch_size: int = ..., use_triton: bool = ..., use_cuda_fp16: bool = ..., autotune_warmup_after_quantized: bool = ..., cache_examples_on_gpu: bool = ...): ...
    @property
    def device(self): ...
    def to(self, device: Union[str, torch.device]): ...
    def forward(self, *args, **kwargs): ...
    def generate(self, **kwargs): ...
    def prepare_inputs_for_generation(self, *args, **kwargs): ...
    def push_to_hub(self, repo_id: str, save_dir: Optional[str] = ..., use_safetensors: Optional[bool] = ..., safetensors_metadata: Optional[Dict[str, str]] = ..., commit_message: Optional[str] = ..., use_auth_token: Optional[Union[bool, str]] = ..., private: Optional[bool] = ..., token: Optional[Union[bool, str]] = ..., create_pr: Optional[bool] = ...) -> str: ...
    def save_quantized(self, save_dir: str, use_safetensors: bool = ..., safetensors_metadata: Optional[Dict[str, str]] = ...): ...
    def save_pretrained(self, save_dir: str, use_safetensors: bool = ..., safetensors_metadata: Optional[Dict[str, str]] = ..., **kwargs): ...
    @classmethod
    def from_pretrained(cls, pretrained_model_name_or_path: str, quantize_config: BaseQuantizeConfig, max_memory: Optional[dict] = ..., trust_remote_code: bool = ..., torch_dtype: torch.dtype = ..., **model_init_kwargs): ...
    @classmethod
    def from_quantized(cls, model_name_or_path: Optional[str], device_map: Optional[Union[str, Dict[str, Union[int, str]]]] = ..., max_memory: Optional[dict] = ..., device: Optional[Union[str, int]] = ..., low_cpu_mem_usage: bool = ..., use_triton: bool = ..., torch_dtype: torch.dtype = ..., inject_fused_attention: bool = ..., inject_fused_mlp: bool = ..., use_cuda_fp16: bool = ..., quantize_config: Optional[BaseQuantizeConfig] = ..., model_basename: Optional[str] = ..., use_safetensors: bool = ..., trust_remote_code: bool = ..., warmup_triton: bool = ..., trainable: bool = ..., **kwargs): ...
    def warmup_triton(self, enabled: bool = ...): ...
    def enable_trainable_mode(self, enabled: bool = ...): ...
    def disable_trainable_mode(self) -> None: ...
    @staticmethod
    def make_sure_compatible_with_peft(model: PreTrainedModel, use_triton: bool, desc_act: bool, group_size: int): ...
    def __getattr__(self, item: Any) -> Any: ...
