from typing import Optional

import torch
from _typeshed import Incomplete

def initialize_model_parallel(tensor_model_parallel_size: int = ..., pipeline_model_parallel_size: int = ..., virtual_pipeline_model_parallel_size: Optional[int] = ..., pipeline_model_parallel_split_rank: Optional[int] = ...) -> None: ...
def initialize_all_reduce_launcher(max_num_tokens: int, hidden_size: int, dtype: torch.dtype, disable_graph: bool = ...) -> None: ...
def model_parallel_is_initialized(): ...
def get_model_parallel_group(): ...
def get_tensor_model_parallel_group(): ...
def get_pipeline_model_parallel_group(): ...
def get_data_parallel_group(): ...
def get_embedding_group(): ...
def get_position_embedding_group(): ...
def set_tensor_model_parallel_world_size(world_size) -> None: ...
def set_pipeline_model_parallel_world_size(world_size) -> None: ...
def get_tensor_model_parallel_world_size(): ...
def get_pipeline_model_parallel_world_size(): ...
def set_tensor_model_parallel_rank(rank) -> None: ...
def set_pipeline_model_parallel_rank(rank) -> None: ...
def set_pipeline_model_parallel_split_rank(rank) -> None: ...
def get_tensor_model_parallel_rank(): ...
def get_pipeline_model_parallel_rank(): ...
def is_pipeline_first_stage(ignore_virtual: bool = ...): ...
def is_pipeline_last_stage(ignore_virtual: bool = ...): ...
def is_rank_in_embedding_group(ignore_virtual: bool = ...): ...
def is_rank_in_position_embedding_group(): ...
def is_pipeline_stage_before_split(rank: Incomplete | None = ...): ...
def is_pipeline_stage_after_split(rank: Incomplete | None = ...): ...
def is_pipeline_stage_at_split(): ...
def get_virtual_pipeline_model_parallel_rank(): ...
def set_virtual_pipeline_model_parallel_rank(rank) -> None: ...
def get_virtual_pipeline_model_parallel_world_size(): ...
def get_tensor_model_parallel_src_rank(): ...
def get_data_parallel_src_rank(): ...
def get_pipeline_model_parallel_first_rank(): ...
def get_pipeline_model_parallel_last_rank(): ...
def get_pipeline_model_parallel_next_rank(): ...
def get_pipeline_model_parallel_prev_rank(): ...
def get_data_parallel_world_size(): ...
def get_data_parallel_rank(): ...
def get_all_reduce_launcher() -> GraphAllReduce: ...
def destroy_model_parallel() -> None: ...

class GraphAllReduce:
    max_num_tokens: Incomplete
    hidden_size: Incomplete
    disable_graph: Incomplete
    group: Incomplete
    buffer: Incomplete
    graphs: Incomplete
    def __init__(self, max_num_tokens: int, hidden_size: int, dtype: torch.dtype, disable_graph: bool = ...) -> None: ...
    def launch(self, x: torch.Tensor) -> torch.Tensor: ...
