from typing import List
from typing import Optional

import torch
from _typeshed import Incomplete
from torch import nn
from vllm import attention_ops as attention_ops
from vllm import cache_ops as cache_ops
from vllm import pos_encoding_ops as pos_encoding_ops
from vllm.model_executor.input_metadata import InputMetadata as InputMetadata

class PagedAttention(nn.Module):
    num_heads: Incomplete
    head_size: Incomplete
    scale: Incomplete
    attn_op: Incomplete
    num_kv_heads: Incomplete
    num_queries_per_kv: Incomplete
    head_mapping: Incomplete
    def __init__(self, num_heads: int, head_size: int, scale: float, num_kv_heads: Optional[int] = ...) -> None: ...
    def set_attn_bias(self, input_metadata: InputMetadata) -> None: ...
    def multi_query_kv_attention(self, output: torch.Tensor, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, input_metadata: InputMetadata) -> torch.Tensor: ...
    def single_query_cached_kv_attention(self, output: torch.Tensor, query: torch.Tensor, key_cache: torch.Tensor, value_cache: torch.Tensor, input_metadata: InputMetadata) -> None: ...
    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, key_cache: Optional[torch.Tensor], value_cache: Optional[torch.Tensor], input_metadata: InputMetadata, cache_event: Optional[torch.cuda.Event]) -> torch.Tensor: ...

class PagedAttentionWithRoPE(PagedAttention):
    def __init__(self, num_heads: int, head_size: int, scale: float, rotary_dim: int, max_position: int = ..., base: int = ..., num_kv_heads: Optional[int] = ...) -> None: ...
    def forward(self, positions: torch.Tensor, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, key_cache: torch.Tensor, value_cache: torch.Tensor, input_metadata: InputMetadata, cache_event: Optional[torch.cuda.Event]) -> torch.Tensor: ...

class PagedAttentionWithALiBi(PagedAttention):
    def __init__(self, num_heads: int, head_size: int, scale: float, slopes: List[float]) -> None: ...
    def set_attn_bias(self, input_metadata: InputMetadata) -> None: ...
    def multi_query_kv_attention(self, output: torch.Tensor, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, input_metadata: InputMetadata) -> torch.Tensor: ...
    def single_query_cached_kv_attention(self, output: torch.Tensor, query: torch.Tensor, key_cache: torch.Tensor, value_cache: torch.Tensor, input_metadata: InputMetadata) -> None: ...
